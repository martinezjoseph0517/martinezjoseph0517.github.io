---
title: "Web-crawler used to fetch sentiment Data Set"
format: html
---

[Download full data-set (ZIP)]("C:\Users\marti\OneDrive\Desktop\test\_site\article_pdfs_10302025.zip")

**The Web-crawler:**

```{r}
#| eval: false
# ===============================
# ğŸ“¦ Install & Load Libraries
# ===============================
install.packages(c("chromote", "rvest", "xml2", "dplyr", "purrr", 
                   "stringr", "lubridate"))
library(chromote)
library(rvest)
library(xml2)
library(dplyr)
library(purrr)
library(stringr)
library(lubridate)

# ===============================
# ğŸ—“ï¸ Define Date Ranges (Janâ€“Oct 2025)
# ===============================
start_dates <- seq(ymd("2025-01-01"), ymd("2025-10-01"), by = "1 month")
end_dates   <- c(start_dates[-1] - days(1), ymd("2025-10-29"))

# ===============================
# ğŸ§  Chromote Article Scraper
# ===============================
get_article_text_chromote <- function(url, wait_time = 3) {
  b <- ChromoteSession$new()
  on.exit(b$close(), add = TRUE)

  tryCatch({
    b$Page$navigate(url)
    Sys.sleep(wait_time)
    html <- b$DOM$getDocument()
    content <- b$DOM$getOuterHTML(html$root$nodeId)
    page <- read_html(content$outerHTML)

    # Multiple possible text containers
    text_nodes <- page %>%
      html_nodes("article p, div[itemprop='articleBody'] p, div.article-body p, section p, p") %>%
      html_text(trim = TRUE)

    text <- paste(unique(text_nodes), collapse = " ")
    if (nchar(text) < 400) {
      message(paste("âš ï¸ Too little text from:", url))
      return(NA)
    }
    return(text)
  }, error = function(e) {
    message(paste("âŒ Failed to load:", url))
    return(NA)
  })
}

# ===============================
# ğŸ•·ï¸ Function to Fetch One RSS Window
# ===============================
get_rss_data <- function(start, end, query = "United States tourism") {
  rss_url <- paste0(
    "https://news.google.com/rss/search?q=",
    URLencode(query),
    "+after:", start, "+before:", end,
    "&hl=en-US&gl=US&ceid=US:en"
  )

  xml <- tryCatch(read_xml(rss_url), error = function(e) NULL)
  if (is.null(xml)) return(tibble())

  tibble(
    title = xml_text(xml_find_all(xml, "//item/title")),
    link  = xml_text(xml_find_all(xml, "//item/link")),
    pubDate = xml_text(xml_find_all(xml, "//item/pubDate")),
    range_start = start,
    range_end = end
  )
}

# ===============================
# ğŸ“ Create Output Folder
# ===============================
dir.create("article_pdfs", showWarnings = FALSE)

# ===============================
# ğŸ” Crawl Loop (All Date Windows)
# ===============================
news_data <- map2_df(start_dates, end_dates, function(s, e) {
  message(paste0("\nğŸ—“ï¸ Fetching articles between ", s, " and ", e, " ..."))
  Sys.sleep(2)
  get_rss_data(s, e)
})

# Clean Google redirect links
news_data <- news_data %>%
  mutate(real_link = ifelse(
    str_detect(link, "google.com/url\\?q="),
    str_extract(link, "(?<=q=)[^&]+"),
    link
  )) %>%
  distinct(real_link, .keep_all = TRUE)

# ===============================
# ğŸ§¾ Scrape Text + Save PDFs
# ===============================
for (i in seq_len(nrow(news_data))) {
  title <- news_data$title[i]
  date <- news_data$pubDate[i]
  link <- news_data$real_link[i]

  message(paste0("\nğŸ” [", i, "/", nrow(news_data), "] ", title))
  Sys.sleep(2)  # polite delay

  article_text <- get_article_text_chromote(link)
  if (is.na(article_text)) next

  # Clean title for file name
  file_title <- make.names(substr(title, 1, 60))
  pdf_path <- file.path("article_pdfs", paste0(file_title, ".pdf"))

  # Create PDF
  pdf(pdf_path, width = 8.5, height = 11)
  par(mar = c(1, 1, 1, 1))
  plot.new()

  header <- paste0("ğŸ“° ", title, "\n\nğŸ“… ", date, "\n\nğŸ”— ", link, "\n\n")
  wrapped <- strwrap(paste(header, article_text), width = 90)
  text(0, 1, paste(wrapped, collapse = "\n"), adj = c(0, 1),
       family = "mono", cex = 0.7)
  dev.off()

  message(paste("âœ… Saved PDF:", pdf_path))
}

# ===============================
# ğŸ’¾ Save Master CSV
# ===============================
write.csv(news_data, "us_tourism_articles_2025.csv", row.names = FALSE)
cat("\nğŸ‰ Crawl complete! PDFs and master CSV saved.\n")


```

```{r}
#| eval: false
#| echo: false
# ===============================
# Install & Load Libraries
# ===============================
install.packages(c("chromote", "rvest", "dplyr", "purrr", "stringr", "lubridate"))
library(chromote)
library(rvest)
library(dplyr)
library(purrr)
library(stringr)
library(lubridate)

# ===============================
# Define Search Query & RSS URL
# ===============================
q <- "United States tourism"
url <- paste0(
  "https://news.google.com/rss/search?q=",
  URLencode(q),
  "&hl=en-US&gl=US&ceid=US:en"
)

# ===============================
# Scrape RSS Feed
# ===============================
rss <- read_xml(url)

news_data <- tibble(
  title = rss %>% xml_find_all("//item/title") %>% xml_text(),
  link = rss %>% xml_find_all("//item/link") %>% xml_text(),
  pubDate = rss %>% xml_find_all("//item/pubDate") %>% xml_text()
)

# ===============================
# Clean Google Redirect Links
# ===============================
news_data <- news_data %>%
  mutate(real_link = ifelse(
    str_detect(link, "google.com/url\\?q="),
    str_extract(link, "(?<=q=)[^&]+"),
    link
  ))

# ===============================
# Define Chromote Article Scraper
# Prototype to real scrapper
# It actualy
# ===============================
get_article_text_chromote <- function(url, wait_time = 3) {
  b <- ChromoteSession$new()
  on.exit(b$close(), add = TRUE)
  
  tryCatch({
    b$Page$navigate(url)
    Sys.sleep(wait_time)  # wait for JavaScript to load
    html <- b$DOM$getDocument()
    content <- b$DOM$getOuterHTML(html$root$nodeId)
    
    page <- read_html(content$outerHTML)
    
    # Try multiple text containers
    text_nodes <- page %>%
      html_nodes("article p, div[itemprop='articleBody'] p, div.article-body p, section p, p") %>%
      html_text(trim = TRUE)
    
    text <- paste(unique(text_nodes), collapse = " ")
    
    if (nchar(text) < 500) {
      message(paste("âš ï¸ Too little text from:", url))
      return(NA)
    }
    
    return(text)
  }, error = function(e) {
    message(paste("âŒ Failed to load:", url))
    return(NA)
  })
}

# ===============================
#  Create Output Folder
# ===============================
dir.create("article_pdfs", showWarnings = FALSE)

# ===============================
#  Crawl & Save Each Article to PDF
# ===============================
for (i in seq_len(nrow(news_data))) {
  link <- news_data$real_link[i]
  title <- news_data$title[i]
  date <- news_data$pubDate[i]
  
  message(paste0("\nğŸ” Scraping article ", i, " of ", nrow(news_data), ": ", title))
  
  Sys.sleep(2)  # polite pause
  article_text <- get_article_text_chromote(link)
  
  if (is.na(article_text)) next  # skip failed scrapes
  
  # Clean title for file name
  file_title <- make.names(substr(title, 1, 50))
  pdf_path <- file.path("article_pdfs", paste0(file_title, ".pdf"))
  
  # Create PDF with article text
  pdf(pdf_path, width = 8.5, height = 11)
  par(mar = c(1, 1, 1, 1))
  plot.new()
  
  title_block <- paste0("ğŸ“° ", title, "\n\nğŸ“… ", date, "\n\nğŸ”— ", link, "\n\n")
  wrapped <- strwrap(paste(title_block, article_text), width = 90)
  text(0, 1, paste(wrapped, collapse = "\n"), adj = c(0, 1), family = "mono", cex = 0.7)
  
  dev.off()
  message(paste("âœ… Saved PDF:", pdf_path))
}

# ===============================
# Optional: Save Master CSV
# ===============================
write.csv(news_data, "us_tourism_news.csv", row.names = FALSE)
message("ğŸ‰ Crawl complete! PDFs and CSV saved.")


```

```{r}
#| eval: false
#| echo: false
# ===============================
# ğŸ“¦ Load libraries
# ===============================
library(rvest)
library(xml2)
library(dplyr)
library(purrr)
library(lubridate)
library(stringr)

# ===============================
# ğŸ—“ï¸ Define date windows (monthly)
# ===============================
start_dates <- seq(ymd("2025-01-01"), ymd("2025-10-01"), by = "1 month")
end_dates   <- c(start_dates[-1] - days(1), ymd("2025-10-29"))

# ===============================
# ğŸ•·ï¸ Function to fetch one RSS feed
# ===============================
get_rss_data <- function(start, end) {
  # Build the Google News RSS URL
  query <- paste0(
    "https://news.google.com/rss/search?q=tourism+after:",
    start, "+before:", end, "&hl=en-US&gl=US&ceid=US:en"
  )
  
  # Read RSS and parse safely
  xml <- tryCatch(read_xml(query), error = function(e) NULL)
  if (is.null(xml)) return(tibble())  # skip failed pulls
  
  tibble(
    title = xml_text(xml_find_all(xml, "//item/title")),
    link  = xml_text(xml_find_all(xml, "//item/link")),
    pubDate = xml_text(xml_find_all(xml, "//item/pubDate")),
    range_start = start,
    range_end = end
  )
}

# ===============================
# ğŸ” Loop through date windows
# ===============================
news_data <- map2_df(start_dates, end_dates, function(s, e) {
  Sys.sleep(2) # polite pause
  get_rss_data(s, e)
})

# ===============================
# ğŸ§¹ Clean and deduplicate
# ===============================
news_data <- news_data %>%
  distinct(link, .keep_all = TRUE) %>%
  mutate(pubDate = as_datetime(pubDate))

# ===============================
# ğŸ’¾ Save to CSV
# ===============================
write.csv(news_data, "tourism_news_Jan_to_Oct_2025.csv", row.names = FALSE)

# ===============================
# ğŸª Preview
# ===============================
print(news_data %>% select(title, pubDate, link) %>% head(10))
cat("\nâœ… Total articles fetched:", nrow(news_data), "\n")

```
